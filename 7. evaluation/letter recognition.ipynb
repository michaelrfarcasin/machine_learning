{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56105f8c-4796-4dbe-bedc-2587e9e8a2c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ce282c-bd56-4267-af60-9e8c5a39a84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://archive.ics.uci.edu/dataset/59/letter+recognition\n",
    "# first used in \"Letter Recognition Using Holland-Style Adaptive Classifiers\"\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "\n",
    "# fetch dataset \n",
    "letter_recognition = fetch_ucirepo(id=59) \n",
    "  \n",
    "# data (as pandas dataframes) \n",
    "X_data = letter_recognition.data.features \n",
    "y_data = letter_recognition.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143b618c-1094-47e1-9f4d-cad14a7cb065",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_data.to_numpy()\n",
    "y = y_data.to_numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b9f668-5dc3-472d-a27c-c040cc81c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_letter_array_to_numbers(letters):\n",
    "    letter_to_number = {chr(i): i - ord('A') + 1 for i in range(ord('A'), ord('Z') + 1)}\n",
    "    return np.array([letter_to_number.get(letter, 0) for letter in letters])\n",
    "\n",
    "def map_number_array_to_letters(numbers):\n",
    "    number_to_letter = {int(c): chr(c + ord('A') - 1) for c in range(26)}\n",
    "    return np.array([number_to_letter.get(number, 'A') for number in numbers])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be52f144-be5a-4215-afbf-9a2cb25795db",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_num = map_letter_array_to_numbers(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9a2378-7f8f-4023-802e-70759b4a00be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get 60% of the dataset as the training set. Put the remaining 40% in temporary variables: x_ and y_.\n",
    "x_train, x_, y_train, y_ = train_test_split(X, y_num, test_size=0.40, random_state=1)\n",
    "\n",
    "# Split the 40% subset above into two: one half for cross validation and the other for the test set\n",
    "x_cv, x_test, y_cv, y_test = train_test_split(x_, y_, test_size=0.50, random_state=1)\n",
    "\n",
    "# Delete temporary variables\n",
    "del x_, y_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff75565f-10ad-4e33-8cdc-544fc40ca5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(x_train[0]) # note the values are all on the same order, so I skip normalization\n",
    "print(y[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe2d6e4-b0a1-46c9-842d-26854867fbbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(16,)),\n",
    "        Dense(16, activation = 'relu'),\n",
    "        Dense(27, activation = 'linear')\n",
    "    ],\n",
    "    name='model_1'\n",
    ")\n",
    "model_2 = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(16,)),\n",
    "        Dense(100, activation = 'relu'),\n",
    "        Dense(60, activation = 'relu'),\n",
    "        Dense(27, activation = 'linear')\n",
    "    ],\n",
    "    name='model_2'\n",
    ")\n",
    "model_3 = Sequential(\n",
    "    [\n",
    "        tf.keras.Input(shape=(16,)),\n",
    "        Dense(25, activation = 'relu'),\n",
    "        Dense(15, activation = 'relu'),\n",
    "        Dense(27, activation = 'linear')\n",
    "    ],\n",
    "    name='model_3'\n",
    ")\n",
    "nn_models = [model_1, model_2, model_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54067424-d1db-4146-8f37-9383ec1582ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_error(model, x, y):\n",
    "    yhat_all_features = model.predict(x)\n",
    "    yhat = np.zeros(yhat_all_features.shape[0])\n",
    "    for i in range(yhat_all_features.shape[0]):\n",
    "        yhat[i] = np.argmax(yhat_all_features[i])\n",
    "    return np.mean(yhat != y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55631036-4aa9-4b26-8db8-98d5bb67d8c8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Initialize lists that will contain the errors for each model\n",
    "nn_train_error = []\n",
    "nn_cv_error = []\n",
    "\n",
    "# Loop over the the models\n",
    "for model in nn_models:\n",
    "    # Setup the loss and optimizer\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.01),\n",
    "    )\n",
    "    print(f\"Training {model.name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(\n",
    "        x_train, \n",
    "        y_train,\n",
    "        epochs=50,\n",
    "        verbose=0\n",
    "    )\n",
    "    print(\"Done!\\n\")\n",
    "    \n",
    "    # Record the fraction of misclassified examples for the training set\n",
    "    train_error = classification_error(model, x_train, y_train)\n",
    "    nn_train_error.append(train_error)\n",
    "    \n",
    "    # Record the fraction of misclassified examples for the cross validation set\n",
    "    cv_error = classification_error(model, x_cv, y_cv)\n",
    "    nn_cv_error.append(cv_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb19b1f-8087-4b73-acc5-ad83f8faafc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the error results\n",
    "for model_num in range(len(nn_train_error)):\n",
    "    print(\n",
    "        f\"Model {model_num+1} ({nn_models[model_num].name}): Training Set Classification Error: {nn_train_error[model_num]:.5f}, \" +\n",
    "        f\"CV Set Classification Error: {nn_cv_error[model_num]:.5f}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e063c2b-0110-444a-b4f2-8afb79cacc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model with the lowest error\n",
    "model_num = np.argmin(nn_cv_error)\n",
    "model = nn_models[model_num]\n",
    "print(f\"Model {model_num+1} ({model.name}) has the lowest error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059f2c48-be38-44a0-85a4-96ffdabd2e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the test error\n",
    "nn_test_error = classification_error(model, x_train, y_train)\n",
    "print(f\"Test error: {nn_test_error}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32bacc6-7c73-46d6-a2d7-52a3a877df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('letter_recognition_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042e8216-f571-4f0b-9c26-a08d2e110c7c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "prediction = model.predict(x_test[:1])\n",
    "yhat = np.argmax(prediction)\n",
    "print(yhat)\n",
    "print(y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c698ad-75f1-4ed2-ae0f-0cf5774faa24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
