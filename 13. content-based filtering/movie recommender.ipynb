{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02bea58-e773-4f04-91e8-cbd806af56a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.ma as ma\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tabulate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bd8606-6b5f-4720-a47e-4e03bc2b574b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://grouplens.org/datasets/movielens/latest/\n",
    "ratings_df = pd.read_csv('./ratings.csv', delimiter=',')\n",
    "movies_df = pd.read_csv('./movies.csv')\n",
    "tags_df = pd.read_csv('./tags.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c732c0ca-7a15-418a-b69e-f1b6211df81a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine genres + tags into one feature text per movie\n",
    "# Merge tags per movie into a single string\n",
    "tags_grouped = tags_df.groupby('movieId')['tag'].apply(lambda x: '|'.join(x)).reset_index()\n",
    "\n",
    "# Merge with movies_df (genres are part of movies)\n",
    "movies_with_tags = pd.merge(movies_df, tags_grouped, on='movieId', how='left')\n",
    "movies_with_tags['tag'] = movies_with_tags['tag'].fillna('')\n",
    "movies_with_tags['features'] = movies_with_tags['genres'].str.lower() + '|' + movies_with_tags['tag'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5880c2bb-861e-444a-abb9-9e10deb4218e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure we have a mapping movieId -> row index in movies_with_tags\n",
    "movie_id_to_index = {mid: i for i, mid in enumerate(movies_with_tags['movieId'].values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74686dee-bcb5-4609-ba4c-7bc99517a6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot / bag-of-words representation for movie features\n",
    "vectorizer = CountVectorizer(token_pattern=r'[^|]+')\n",
    "X_m = vectorizer.fit_transform(movies_with_tags['features'])\n",
    "feature_names = vectorizer.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a200f00-fb5f-497b-adde-bf634c924224",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('features.txt', feature_names, fmt='%s', newline='\\n') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dcb6bed-baef-4283-b356-90dd965ac7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user feature matrix X_u\n",
    "# We'll take weighted average of movie features by user ratings\n",
    "num_users = ratings_df['userId'].nunique()\n",
    "num_features = X_m.shape[1]\n",
    "X_u = np.zeros((num_users, num_features))\n",
    "\n",
    "for uid, group in ratings_df.groupby('userId'):\n",
    "    rated_movies = group['movieId'].values\n",
    "    ratings = group['rating'].values\n",
    "    \n",
    "    # filter to keep only movies that exist in movie_id_to_index and preserve the rating order\n",
    "    movie_to_rating_pairs = [(mid, r) for mid, r in zip(rated_movies, ratings) if mid in movie_id_to_index]\n",
    "    if not movie_to_rating_pairs:\n",
    "        continue\n",
    "    movie_ids, weights = zip(*movie_to_rating_pairs)\n",
    "    movie_indices = [movie_id_to_index[movie_id] for movie_id in movie_ids]\n",
    "    \n",
    "    movie_features = X_m[movie_indices].toarray()\n",
    "    user_profile = np.average(movie_features, axis = 0, weights = np.array(weights))\n",
    "    X_u[uid - 1] = user_profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a847a85-1b8b-4a8e-83c5-53076e471ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each rating row, look up the user's profile and the movie's feature vector\n",
    "user_indices = ratings_df['userId'].values - 1\n",
    "movie_indices = ratings_df['movieId'].map(movie_id_to_index).values\n",
    "# Build aligned (user, item) input arrays â€” one sample per rating row\n",
    "# user_inputs[i] corresponds to item_inputs[i] and y[i]\n",
    "user_inputs = X_u[user_indices]\n",
    "item_inputs = X_m[movie_indices].toarray()\n",
    "y_all = ratings_df['rating'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc322916-7238-43e8-8c25-9b459cef5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('aligned shapes (user_inputs, item_inputs, y):', user_inputs.shape, item_inputs.shape, y_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc340223-0cff-4d87-a368-6538adedfc5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single train/test split applied to aligned arrays\n",
    "user_train, user_test, item_train, item_test, y_train, y_test = \\\n",
    "    train_test_split(user_inputs, item_inputs, y_all, train_size=0.8, shuffle=True)\n",
    "\n",
    "# Convert dtypes to float32 (TF/Keras friendly)\n",
    "user_train = user_train.astype(np.float32)\n",
    "item_train = item_train.astype(np.float32)\n",
    "y_train = y_train.astype(np.float32)\n",
    "user_test = user_test.astype(np.float32)\n",
    "item_test = item_test.astype(np.float32)\n",
    "y_test = y_test.astype(np.float32)\n",
    "\n",
    "# Scale features: fit scalers on training data and transform test\n",
    "scalerItem = StandardScaler(with_mean=False)\n",
    "scalerItem.fit(item_train)\n",
    "item_train_scaled = scalerItem.transform(item_train)\n",
    "item_test_scaled = scalerItem.transform(item_test)\n",
    "\n",
    "scalerUser = StandardScaler(with_mean=False)\n",
    "scalerUser.fit(user_train)\n",
    "user_train_scaled = scalerUser.transform(user_train)\n",
    "user_test_scaled = scalerUser.transform(user_test)\n",
    "\n",
    "scalerTarget = MinMaxScaler((-1, 1))\n",
    "scalerTarget.fit(y_train.reshape(-1, 1))\n",
    "y_train_scaled = scalerTarget.transform(y_train.reshape(-1, 1)).reshape(-1)\n",
    "y_test_scaled = scalerTarget.transform(y_test.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e0e8d-f00e-4e6e-98d3-9acc86fc74a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train shapes (user, item, y):', user_train.shape, item_train.shape, y_train.shape)\n",
    "print('test shapes  (user, item, y):', user_test.shape, item_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24311f91-5259-4db0-8f5d-a87f1fd5bc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build neural networks\n",
    "num_outputs = 32\n",
    "user_NN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_outputs)\n",
    "])\n",
    "\n",
    "item_NN = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(num_outputs)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c657ae-c9f3-4e7b-84ee-2c269b3c31b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_user_features = user_train.shape[1]\n",
    "num_item_features = item_train.shape[1]\n",
    "\n",
    "# create the user input and point to the base network\n",
    "input_user = tf.keras.layers.Input(shape=(num_user_features,))\n",
    "vu = user_NN(input_user)\n",
    "vu = tf.keras.layers.Lambda(lambda x: tf.linalg.l2_normalize(x, axis = 1))(vu)\n",
    "\n",
    "# create the item input and point to the base network\n",
    "input_item = tf.keras.layers.Input(shape=(num_item_features,))\n",
    "vm = item_NN(input_item)\n",
    "vm = tf.keras.layers.Lambda(lambda x: tf.linalg.l2_normalize(x, axis = 1))(vm)\n",
    "\n",
    "# compute the dot product of the two vectors vu and vm\n",
    "output = tf.keras.layers.Dot(axes=1)([vu, vm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682d6e1a-a2de-49e1-a3f6-1111d7780327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify the inputs and output of the model\n",
    "model = tf.keras.Model([input_user, input_item], output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aa0523-b242-4ecf-8dc1-d98d8690a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=0.01), \n",
    "    loss = tf.keras.losses.MeanSquaredError()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddeb7a4-6a98-4b79-8c5c-426aecb9f42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(\n",
    "    monitor = 'loss',\n",
    "    patience = 3,\n",
    "    min_delta = 0.0001,\n",
    "    restore_best_weights = True\n",
    ")\n",
    "model.fit(\n",
    "    [user_train, item_train], \n",
    "    y_train, \n",
    "    epochs = 30,\n",
    "    callbacks = [early_stopping_callback]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0595b4-c6b5-4d92-924b-e748c703c22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate([user_test, item_test], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef15f6fa-5d08-47ce-8433-6f3bb0c85a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_to_index = {feat: i for i, feat in enumerate(feature_names)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc19e36-b8d7-4690-8b2f-e657e8432d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new user\n",
    "new_user_profile = np.zeros(num_features)\n",
    "new_user_profile[feature_to_index.get('1990s')] = 4\n",
    "new_user_profile[feature_to_index.get('animation')] = 5\n",
    "new_user_profile[feature_to_index.get('comedy')] = 5\n",
    "new_user_profile[feature_to_index.get('kids')] = 5\n",
    "new_user_profile[feature_to_index.get('adorable')] = 5\n",
    "new_user_profile[feature_to_index.get('funny')] = 5\n",
    "new_user_profile[feature_to_index.get('whimsical')] = 5\n",
    "new_user_profile[feature_to_index.get('romantic')] = 5\n",
    "new_user_profile[feature_to_index.get('romantic comedy')] = 5\n",
    "new_user_profile[feature_to_index.get('macaulay culkin')] = 4\n",
    "new_user_profile[feature_to_index.get('humour')] = 5\n",
    "new_user_profile[feature_to_index.get('great humor')] = 5\n",
    "new_user_profile[feature_to_index.get('cartoon')] = 5\n",
    "new_user_profile[feature_to_index.get('anime')] = 5\n",
    "new_user_profile[feature_to_index.get('disney')] = 5\n",
    "new_user_profile[feature_to_index.get('hayao miyazaki')] = 5\n",
    "new_user_profile[feature_to_index.get('food')] = 5\n",
    "new_user_profile[feature_to_index.get('quirky romantic')] = 5\n",
    "new_user_profile[feature_to_index.get('sentimental')] = 5\n",
    "new_user_profile[feature_to_index.get('adventure')] = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d816ad-fecb-4bd2-b147-e17a2167af73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for the new user\n",
    "user_profile_scaled = scalerUser.transform(new_user_profile.reshape(1, -1))\n",
    "\n",
    "# Prepare item features\n",
    "items_all = X_m.toarray().astype(np.float32)\n",
    "items_all_scaled = scalerItem.transform(items_all)\n",
    "\n",
    "# Duplicate user profile for every item and predict\n",
    "n_items = items_all_scaled.shape[0]\n",
    "users_rep = np.repeat(user_profile_scaled, n_items, axis = 0)\n",
    "preds_scaled = model.predict([users_rep, items_all_scaled], batch_size=1024, verbose=0).reshape(-1)\n",
    "\n",
    "# Undo target scaling to get back to original rating scale\n",
    "predictions = scalerTarget.inverse_transform(preds_scaled.reshape(-1, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286fa95c-4782-4b14-aa91-98aa580f7680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top recommendations\n",
    "K = 20\n",
    "rank_ix = np.argsort(predictions)[::-1]\n",
    "recommendation_indices = [i for i in rank_ix][:K]\n",
    "movie_titles = movies_df.set_index('movieId')['title'].to_dict()\n",
    "\n",
    "# Map row indices -> titles and print\n",
    "print(\"\\nTop recommendations:\")\n",
    "for idx in recommendation_indices:\n",
    "    # movies_with_tags has rows in the same order as X_m / X_m_reduced\n",
    "    try:\n",
    "        movie_id = movies_with_tags.loc[idx, 'movieId']\n",
    "        title = movie_titles.get(movie_id, f\"MovieId {movie_id}\")\n",
    "    except Exception:\n",
    "        # fallback via index_to_movie_id if present\n",
    "        try:\n",
    "            movie_id = index_to_movie_id[idx]\n",
    "            title = movie_titles.get(movie_id, f\"MovieId {movie_id}\")\n",
    "        except Exception:\n",
    "            title = f\"Index {idx}\"\n",
    "    print(f\"{title}: predicted {predictions[idx]:0.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bb63a5-edb4-42f1-b7de-252959508f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All the recommendations have a predicted rated of 5.0. The conclusion I draw from that is that the features I used have too little overlap\n",
    "# i.e. many movies with highly-rated features are missing poorly-rated features and vice-versa\n",
    "# So for this particular user, this model might be less accurate than the collaborative filtering model - at least until the user rates more features\n",
    "# I might also get higher accuracy by reducing the number of features or applying the existing features to more movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "748f6929-d2ec-4af6-9629-58bd08ab79c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
